<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="lecture-4-numerical-and-statistical-integrations">Lecture 4: Numerical and Statistical Integrations</h1>
<p>Zhentao Shi</p>
<p>3/14/2016</p>
<h2 id="computational-alternative-to-analytical-methods">Computational Alternative to Analytical Methods</h2>
<p>Integration and differentiation involve limits in their mathematical definitions. However, a modern computer is a finite-precision machine that can handle neither arbitrarily small nor arbitrarily big numbers. We attempt to bridge the gap by approximation.</p>
<h3 id="numerical-derivatives">Numerical Derivatives</h3>
<p>Numerical derivative is particularly convenient when the analytical alternative is too complex. We do not want to program up those analytical expressions in the trial-and-error stage.</p>
<p>Package <code>numDeriv</code></p>
<ul>
<li><code>grad</code> for a scalar-valued function</li>
<li><code>jacobian</code> for a real-vector-valued function</li>
<li><code>hessian</code> for a scalar-valued function</li>
<li><code>genD</code> for a real-vector-valued function</li>
</ul>
<h3 id="numerical-integration">Numerical Integration</h3>
<ul>
<li>one-dimensional quadrature: <code>integrate</code></li>
<li>multi-dimensional quadrature: <code>adaptIntegrate</code> in the package <code>cubature</code></li>
</ul>
<h2 id="simulated-methods">Simulated Methods</h2>
<p><strong>Motivation example: a structural model with latent variables</strong></p>
<p>In the classical Roy model, a man can assume one of the two occupations: a farmer or a fisher. The utility of being a farmer is <span class="math">\(U_1^{*} = x&#39; \beta_1 + e_1\)</span> and that of being a fisher is <span class="math">\(U_2^{*} = x&#39; \beta_2 + e_2\)</span>, where <span class="math">\(U_1^{*}\)</span> and <span class="math">\(U_2^{*}\)</span> are latent (unobservable). In reality, we observe the binary outcome <span class="math">\(y=\mathbf{1}\{U_1^{*}&gt; U_2^{*}\}\)</span>. If <span class="math">\[
\begin{bmatrix}
e_1\\e_2
\end{bmatrix}
\sim N \left(
\begin{bmatrix}
0 \\ 0
\end{bmatrix},
  \begin{bmatrix}
  \sigma_1^2 &amp; \sigma_{12} \\ \sigma_{12} &amp; 1
  \end{bmatrix}\right)\]</span> where <span class="math">\(\sigma_2^2\)</span> is normalized to be 1, we can write down the log-likelihood of an observable sample as <span class="math">\[L(\theta) = \sum_{i = 1}^n  \left\{ y_i \log P( U_{i1}^* &gt; U_{i2}^* )
+ (1-y_i)\log P( U_{i1}^* \leq  U_{i2}^* ) \right\}.\]</span> The analytical form is complicated because of the presence of correlation in the joint distribution of <span class="math">\((e_1, e_2)\)</span>. What is worse, analytical form may not be obtainable if <span class="math">\((e_1, e_2)\)</span> follows some other distribution.</p>
<p>The key difficulty of the analytical approach lies in the explicit form of <span class="math">\[p(\theta|x_i) = P\left( U^*_{i1}(\theta) &gt; U^*_{i2}(\theta) \right)
= P\left( x_i&#39;(\beta_1 - \beta_2) + e_{i1} - e_{i2} &gt; 0 \right), \]</span> which is complicated because of the correlation between <span class="math">\(e_1\)</span> and <span class="math">\(e_2\)</span>. However, given a trial value <span class="math">\(\theta\)</span>, it is easy to simulate the probability by drawing artificial <span class="math">\((e^s_1, e^s_2)\)</span> from a proposed distribution. In such a simulation approach, we estimate <span class="math">\[
\hat{p}(\theta|x_i) = \frac{1}{S} \sum_{i=1}^S \mathbf{1}\left( U^{s*}_{i1}(\theta) &gt; U^{s*}_{i2}(\theta) \right),
\]</span> where <span class="math">\(s=1,\ldots,S\)</span> is the index of simulation, and <span class="math">\(S\)</span> is the total number of simulation replications.</p>
<h4 id="generate-random-variables">Generate Random Variables</h4>
<p>If the CDF <span class="math">\(F(X)\)</span> is known and <span class="math">\(U\sim \mathrm{Uniform}(0,1)\)</span>, then <span class="math">\(F^{-1}(U)\)</span> follows the distribution <span class="math">\(F(X)\)</span>.</p>
<p>If the pdf <span class="math">\(f(X)\)</span> is known, we can generate a sample with such a distribution by the importance sampling. Metropolis-Hastings algorithm is such a method. R package <code>mcmc</code> implements Metropolis-Hastings algorithm.</p>
<p><strong>Example</strong>: use <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings algorithm</a> to generate a sample of normally distributed observations.</p>
<pre><code>library(mcmc)
h = function(x){ y = -x^2 / 2 } # the log, unnormalized function

out = metrop( obj = h, initial = 0, nbatch = 100, nspac = 1  )
plot(out$batch, type = &quot;l&quot;) # a time series with flat steps

out = metrop( obj = h, initial = 0, nbatch = 100, nspac = 10  )
plot(out$batch, type = &quot;l&quot;) # a time series looks like a white noise

out = metrop( obj = h, initial = 0, nbatch = 10000, nspac = 10  )
summary(out)
plot(density(out$batch))</code></pre>
<h4 id="optimization">Optimization</h4>
<p><a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-Maximization algorithm</a> is an iterative estimation method for statistical models in which likelihood is difficult to express in explicit forms.</p>
<p>It is better to categorize the EM algorithm as a numerical optimization method assisted by simulation.</p>
<h3 id="indirect-inference">Indirect Inference</h3>
<ul>
<li>Economic structural model</li>
<li>Reduced-form: (auxiliary model)</li>
<li>Binding function: a mapping from the parameter space of the reduced-form to that of the structural form.</li>
</ul>
<p><strong>Example</strong>: In the Roy model example, the structural parameter is <span class="math">\(\theta = (\beta, \sigma_1^2, \sigma_{12} )\)</span>. The choice of the reduced-form model is not unique. A sensible reduced-form model is the linear regression between <span class="math">\(y_i\)</span> and <span class="math">\(x_i\)</span>. A set of reduced-form parameters can be chosen as <span class="math">\[
\begin{eqnarray*}
b_1 &amp; = &amp; (X&#39;X)^{-1}X&#39;y \\
b_2 &amp; = &amp; n^{-1}\sum_{y_i=1} (y_i - x_i&#39; b_1)^2 = n^{-1}\sum_{y_i=1} (1 - x_i&#39; b_1)^2  \\
b_2 &amp; = &amp; n^{-1}\sum_{y_i=0} (y_i - x_i&#39; b_1)^2 = n^{-1}\sum_{y_i=0} (x_i&#39; b_1)^2 \\
\end{eqnarray*}
\]</span> where in the binding function <span class="math">\(b_1\)</span> is associated with <span class="math">\(\beta\)</span>, and <span class="math">\((b_2,b_3)\)</span> are associated with <span class="math">\((\sigma_1^2,\sigma_{12})\)</span>.</p>
<p><strong>Example</strong>: linear IV model.</p>
<h3 id="simulated-method-of-moments">Simulated Method of Moments</h3>
<p>Pakes and Pollard (1989): Simulation and the asymptotics of optimization estimators</p>
<p>Match moments generated the theoretical model with their empirical counterparts. The choice of the moments to be matched is not unique either. A set of valid choice is <span class="math">\[
\begin{eqnarray*}
n^{-1} \sum_{i=1}^n x_i (y_i - \hat{p}(\theta | x_i)) &amp;\approx &amp; 0\\
n^{-1} \sum_{i=1}^n (y_i - \bar{y})^2 - \bar{\hat{p}}(\theta) (1- \bar{\hat{p}}(\theta)) &amp;\approx &amp; 0\\
n^{-1} \sum_{i=1}^n (x_i - \bar{x} ) (y_i - \hat{p}(\theta | x_i))^2 &amp;\approx &amp; 0\\
\end{eqnarray*}
\]</span> where <span class="math">\(\bar{y} = n^{-1} \sum_{i=1}^n y_i\)</span> and <span class="math">\(\bar{\hat{p}}(\theta) = n^{-1} \sum_{i=1}^n p(\theta|x_i)\)</span>. The first set of moments is justified by the independence of <span class="math">\((e_{i1}, e_{i2})\)</span> and <span class="math">\(x_i\)</span> so that <span class="math">\(E[x_i y_i] = x_i E[y_i | x_i] = x_i p(\theta|x_i)\)</span>, and the second set matches the variance of <span class="math">\(y_i\)</span>. Moreover, we need to choose a weighting matrix <span class="math">\(W\)</span> to form a quadratic criterion for GMM.</p>
<h3 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h3>
<h4 id="laplace-type-estimator">Laplace-type estimator</h4>
<p>Chernozhukov and Hong (2003): An MCMC approach to classical estimation</p>
<p>If we know the distribution of an extremum estimator, then <em>asymptotically</em> the point estimator equals its mean under the quadratic loss function, and equals its median under the absolute-value loss function.</p>
<p>The <em>Laplace-type estimator</em> (LTE) transforms the value of the criterion function of an extremum estimator into a probability weight. In a minimization problem, the smaller is the value of the criterion function, the larger it weighs.</p>
<p><strong>Example</strong>: LTE estimation for linear regression</p>
<pre><code>library(mcmc)

# DGP
n = 100
b0 = c(1,2)
X = cbind(1, rnorm(n))
Y = X %*% b0 + rnorm(n)

# Laplace-type estimator
L = function(b) -sum( (Y - X %*% b )^2 )  # criterion function

out = metrop( obj = L, initial = c(0,0), nbatch = 10000, nspac = 10  )

# summarize the estimation
bhat2 = out$batch[,2]
bhat2_point = mean(bhat2)
bhat2_var   = var(bhat2)
bhat2_CI = quantile(bhat2, c(.025, .975) )

# compare with OLS
b_OLS = summary( lm(Y~-1+X) )</code></pre>
<h3 id="bayes-estimation">Bayes Estimation</h3>
<p>Example: linear regression with normal error.</p>
<h4 id="gibbs-sampling">Gibbs sampling</h4>
<p>Gibbs sampling is an MCMC method that is used to generate a multivariate distribution when its marginal distribution of each component is easy to calculate.</p>
<p>Application: the linear regression model with many coefficients.</p>
<h2 id="extended-readings">Extended Readings</h2>
<ul>
<li>Arcidiacono and Jones (2003): <a href="http://www.jstor.org/stable/1555527?seq=1#page_scan_tab_contents">Finite Mixture Distributions, Sequential Likelihood and the EM Algorithm</a></li>
<li>Gourieroux, Monfort and Renault (1993): <a href="http://onlinelibrary.wiley.com/doi/10.1002/jae.3950080507/abstract">Infirect Inference</a></li>
</ul>
</body>
</html>
