{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Numerical and Statistical Integrations\n",
    "\n",
    "Zhentao Shi\n",
    "\n",
    "3/14/2016\n",
    "\n",
    "## Computational Alternative to Analytical Methods\n",
    "\n",
    "Integration and differentiation involve limits in their mathematical definitions. However, a modern computer is a finite-precision machine that can handle neither arbitrarily small nor arbitrarily big numbers. We attempt to bridge the gap by approximation.\n",
    "\n",
    "### Numerical Derivatives\n",
    "\n",
    "Numerical derivative is particularly convenient when the analytical alternative is\n",
    "too complex. We do not want to program up those analytical expressions in the\n",
    "trial-and-error stage.\n",
    "\n",
    "Package `numDeriv`\n",
    "\n",
    "* `grad` for a scalar-valued function\n",
    "* `jacobian` for a real-vector-valued function\n",
    "* `hessian` for a scalar-valued function\n",
    "* `genD` for a real-vector-valued function\n",
    "\n",
    "\n",
    "### Numerical Integration\n",
    "\n",
    "* one-dimensional quadrature: `integrate`\n",
    "* multi-dimensional quadrature: `adaptIntegrate` in the package `cubature`\n",
    "\n",
    "## Simulated Methods\n",
    "\n",
    "**Motivation example: a structural model with latent variables**\n",
    "\n",
    "In the classical Roy model, a man can assume one of the two occupations: a farmer or a fisher. The utility of being a farmer is $U_1^{*} = x' \\beta_1 + e_1$ and that of being a fisher is $U_2^{*} = x' \\beta_2 + e_2$, where $U_1^{*}$ and $U_2^{*}$ are latent (unobservable). In reality, we observe the binary outcome $y=\\mathbf{1}\\{U_1^{*}> U_2^{*}\\}$. If\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e_1\\\\e_2\n",
    "\\end{bmatrix}\n",
    "\\sim N \\left(\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0\n",
    "\\end{bmatrix},\n",
    "  \\begin{bmatrix}\n",
    "  \\sigma_1^2 & \\sigma_{12} \\\\ \\sigma_{12} & 1\n",
    "  \\end{bmatrix}\\right)$$\n",
    "where $\\sigma_2^2$ is normalized to be 1, we can write down the log-likelihood of an observable sample as\n",
    "$$L(\\theta) = \\sum_{i = 1}^n  \\left\\{ y_i \\log P( U_{i1}^* > U_{i2}^* )\n",
    "+ (1-y_i)\\log P( U_{i1}^* \\leq  U_{i2}^* ) \\right\\}.$$\n",
    "The analytical form is complicated because of the presence of correlation in the joint distribution of $(e_1, e_2)$. What is worse, analytical form may not be obtainable if $(e_1, e_2)$ follows some other distribution.\n",
    "\n",
    "The key difficulty of the analytical approach lies in the explicit form of\n",
    "$$p(\\theta|x_i) = P\\left( U^*_{i1}(\\theta) > U^*_{i2}(\\theta) \\right)\n",
    "= P\\left( x_i'(\\beta_1 - \\beta_2) + e_{i1} - e_{i2} > 0 \\right), $$\n",
    "which is complicated because of the correlation between $e_1$ and $e_2$.\n",
    "However, given a trial value $\\theta$, it is easy to simulate the probability by drawing artificial $(e^s_1, e^s_2)$ from a proposed distribution. In such a simulation approach, we estimate\n",
    "$$\n",
    "\\hat{p}(\\theta|x_i) = \\frac{1}{S} \\sum_{i=1}^S \\mathbf{1}\\left( U^{s*}_{i1}(\\theta) > U^{s*}_{i2}(\\theta) \\right),\n",
    "$$\n",
    "where $s=1,\\ldots,S$ is the index of simulation, and $S$ is the total number of simulation replications.\n",
    "\n",
    "#### Generate Random Variables\n",
    "\n",
    "If the CDF $F(X)$ is known and $U\\sim \\mathrm{Uniform}(0,1)$, then $F^{-1}(U)$ follows the distribution $F(X)$.\n",
    "\n",
    "If the pdf $f(X)$ is known, we can generate a sample with such a distribution by the importance sampling.\n",
    "Metropolis-Hastings algorithm is such a method.\n",
    "R package `mcmc` implements Metropolis-Hastings algorithm.\n",
    "\n",
    "**Example**: use [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) to generate a sample of normally distributed observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(mcmc)\n",
    "h = function(x){ y = -x^2 / 2 } # the log, unnormalized function\n",
    "\n",
    "out = metrop( obj = h, initial = 0, nbatch = 100, nspac = 1  )\n",
    "plot(out$batch, type = \"l\") # a time series with flat steps\n",
    "\n",
    "out = metrop( obj = h, initial = 0, nbatch = 100, nspac = 10  )\n",
    "plot(out$batch, type = \"l\") # a time series looks like a white noise\n",
    "\n",
    "out = metrop( obj = h, initial = 0, nbatch = 10000, nspac = 10  )\n",
    "summary(out)\n",
    "plot(density(out$batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "[Expectation-Maximization algorithm](http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) is an iterative estimation method for statistical models in which likelihood is difficult to express in explicit forms.\n",
    "\n",
    "It is better to categorize the EM algorithm as a numerical optimization method assisted by simulation.\n",
    "\n",
    "### Indirect Inference\n",
    "\n",
    "* Economic structural model\n",
    "* Reduced-form: (auxiliary model)\n",
    "* Binding function: a mapping from the parameter space of the reduced-form to that\n",
    "of the structural form.\n",
    "\n",
    "**Example**: In the Roy model example, the structural parameter is $\\theta = (\\beta, \\sigma_1^2, \\sigma_{12} )$. The choice of the reduced-form model is not unique.\n",
    "A sensible reduced-form model is the linear regression between $y_i$ and $x_i$.\n",
    "A set of reduced-form parameters can be chosen as\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "b_1 & = & (X'X)^{-1}X'y \\\\\n",
    "b_2 & = & n^{-1}\\sum_{y_i=1} (y_i - x_i' b_1)^2 = n^{-1}\\sum_{y_i=1} (1 - x_i' b_1)^2  \\\\\n",
    "b_2 & = & n^{-1}\\sum_{y_i=0} (y_i - x_i' b_1)^2 = n^{-1}\\sum_{y_i=0} (x_i' b_1)^2 \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where in the binding function $b_1$ is associated with $\\beta$, and $(b_2,b_3)$ are associated with $(\\sigma_1^2,\\sigma_{12})$.\n",
    "\n",
    "**Example**: linear IV model.\n",
    "\n",
    "\n",
    "### Simulated Method of Moments\n",
    "\n",
    "Pakes and Pollard (1989): Simulation and the asymptotics of optimization estimators\n",
    "\n",
    "Match moments generated the theoretical model with their empirical counterparts. The choice of the moments to be matched is not unique either. A set of valid choice is\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "n^{-1} \\sum_{i=1}^n x_i (y_i - \\hat{p}(\\theta | x_i)) &\\approx & 0\\\\\n",
    "n^{-1} \\sum_{i=1}^n (y_i - \\bar{y})^2 - \\bar{\\hat{p}}(\\theta) (1- \\bar{\\hat{p}}(\\theta)) &\\approx & 0\\\\\n",
    "n^{-1} \\sum_{i=1}^n (x_i - \\bar{x} ) (y_i - \\hat{p}(\\theta | x_i))^2 &\\approx & 0\\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where $\\bar{y} = n^{-1} \\sum_{i=1}^n y_i$ and\n",
    "$\\bar{\\hat{p}}(\\theta) = n^{-1} \\sum_{i=1}^n p(\\theta|x_i)$.\n",
    "The first set of moments is justified by the independence of $(e_{i1}, e_{i2})$ and $x_i$ so that $E[x_i y_i] = x_i E[y_i | x_i] = x_i p(\\theta|x_i)$, and the second set matches the variance of $y_i$.\n",
    "Moreover, we need to choose a weighting matrix $W$ to form a quadratic criterion for GMM.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Markov Chain Monte Carlo\n",
    "\n",
    "#### Laplace-type estimator\n",
    "\n",
    "Chernozhukov and Hong (2003): An MCMC approach to classical estimation\n",
    "\n",
    "\n",
    "If we know the distribution of an extremum estimator, then *asymptotically* the point estimator equals its mean under the quadratic loss function, and equals its median under the absolute-value loss function.\n",
    "\n",
    "The *Laplace-type estimator* (LTE) transforms the value of the criterion function of an extremum estimator into a probability weight. In a minimization problem, the smaller is the value of the criterion function, the larger it weighs.\n",
    "\n",
    "**Example**: LTE estimation for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(mcmc)\n",
    "\n",
    "# DGP\n",
    "n = 100\n",
    "b0 = c(1,2)\n",
    "X = cbind(1, rnorm(n))\n",
    "Y = X %*% b0 + rnorm(n)\n",
    "\n",
    "# Laplace-type estimator\n",
    "L = function(b) -sum( (Y - X %*% b )^2 )  # criterion function\n",
    "\n",
    "out = metrop( obj = L, initial = c(0,0), nbatch = 10000, nspac = 10  )\n",
    "\n",
    "# summarize the estimation\n",
    "bhat2 = out$batch[,2]\n",
    "bhat2_point = mean(bhat2)\n",
    "bhat2_var   = var(bhat2)\n",
    "bhat2_CI = quantile(bhat2, c(.025, .975) )\n",
    "\n",
    "# compare with OLS\n",
    "b_OLS = summary( lm(Y~-1+X) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Readings\n",
    "\n",
    "* Arcidiacono and Jones (2003): [Finite Mixture Distributions, Sequential Likelihood and the EM Algorithm](http://www.jstor.org/stable/1555527?seq=1#page_scan_tab_contents)\n",
    "* Gourieroux, Monfort and Renault (1993): [Infirect Inference](http://onlinelibrary.wiley.com/doi/10.1002/jae.3950080507/abstract)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
