#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand\theenumi{(\alph{enumi})}
\renewcommand\labelenumi{\theenumi}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format dvi
\output_sync 1
\output_sync_macro "\synctex=-1"
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\branch abc
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\branch pf of gamma = 0
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Optimization is a core technique widely used in science and engineering.
 In engineering, numerical optimization usually refers to deterministic
 optimization.
 The numbers to be optimized is called the variable, while the numbers involved
 in the optimization problem is called the parameters.
 Here we will stick to the econometric terminology.
\end_layout

\begin_layout Standard
These terminologies are exactly the opposite as they are used in econometrics
 as well as statistics.
 Econometrics views that observed data as a sample draw from a population
 distribution 
\begin_inset Formula $P\left(\theta\right)$
\end_inset

, where 
\begin_inset Formula $\theta$
\end_inset

 is the parameter.
 In practice 
\begin_inset Formula $\theta$
\end_inset

 is unknown, and the researcher is interested in learning 
\begin_inset Formula $\theta$
\end_inset

 from the observed data, which are called the (random) variables.
 
\end_layout

\begin_layout Standard
A large number of econometric estimators can be classified as a member of
 the extremum estimators.
 The name indicates they are the optimizer of either a minimization problem
 or a maximization problem.
 In econometrics, we care about how we actually obtain the optimizer.
 Moreover, we pay much attention about the stochastic properties of the
 optimizer.
 As a function of random variables, the optimizer of an econometric problem
 is also a random variable.
 We are interested to characterize the uncertainty of the estimator.
 This is a key difference from the deterministic optimization in engineering.
\end_layout

\begin_layout Standard
A general optimization problem is formulated as 
\begin_inset Formula 
\[
\min_{\theta\in\Theta}f(\theta)\mbox{ \ \ s.t. }g(\theta)=0,\ h(\theta)\leq0,
\]

\end_inset

where 
\begin_inset Formula $f(\cdot)$
\end_inset

 is a criterion function, 
\begin_inset Formula $g(\theta)=0$
\end_inset

 is an equality constraint, and 
\begin_inset Formula $h(\theta)\leq0$
\end_inset

 is an inequality constraint.
 In econometric either 
\begin_inset Formula $f$
\end_inset

, 
\begin_inset Formula $g$
\end_inset

 and 
\begin_inset Formula $h$
\end_inset

 may depend on the sample.
\end_layout

\begin_layout Section
Deterministic Optimization
\end_layout

\begin_layout Standard
First, we condition on all the other variables except the parameters to
 be estimated.
 The concern is exactly as the engineers: how we can obtain the optimizer
 given the other numbers.
 
\end_layout

\begin_layout Standard
Up to now, most established numerical optimization algorithm can a local
 minimum if it exists.
 However, there is no guarantee to locate the global minimum when multiple
 local minima exist.
 
\end_layout

\begin_layout Standard
* unconstrained or constrained
\end_layout

\begin_layout Standard
Popular algorithms
\end_layout

\begin_layout Standard
* Newton-type algorithm - gradient - Hessian * Quasi-Newton type algorithm
\end_layout

\begin_layout Standard
**Example**: pseudo Poisson maximum likelihood
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $y_{i}$
\end_inset

 is a continuous random variable, it obviously does not follow a possion
 distribution, whose support is non-negative integers.
 However, if the conditional mean model 
\begin_inset Formula $E[y_{i}|x_{i}]=\exp(x_{i}\beta),$
\end_inset

 is stratified, we can still use the possion regression to obtain a consistent
 estimator of the parameter 
\begin_inset Formula $\beta$
\end_inset

 even if 
\begin_inset Formula $y_{i}$
\end_inset

 does not follow a conditional poisson distribution, 
\end_layout

\begin_layout Standard
To implement optimization in `R`, the criterion must be written as a function
 of a sole argument--the optimization parameter.
 All data must be provided inside or outside of the function, but not via
 any additional argument.
\end_layout

\begin_layout Section
Convex optimization
\end_layout

\begin_layout Standard
If a function is convex in its argument, then a local minimum is a global
 minimum.
 Convex optimization is particularly important in high-dimensional problems.
 
\end_layout

\begin_layout Standard
**Example**
\end_layout

\begin_layout Standard
* linear regression model MLE 
\end_layout

\begin_layout Standard
* Lasso 
\end_layout

\begin_layout Standard
* empirical likelihood
\end_layout

\begin_layout Standard
[Rmosek](http://rmosek.r-forge.r-project.org/) is an interface in R to access
 Mosek, a high-quality commercial solver dedicated to convex optimization.
 Mosek provides free academic licenses.
\end_layout

\begin_layout Section
Extended Readings
\end_layout

\begin_layout Standard
Boyd and Vandenberghe (2004): [Convex Optimization](http://stanford.edu/~boyd/cvx
book/)
\end_layout

\begin_layout Standard
Buhlmann and van de Geer (2011): [Statistics for High-Dimensional Data](http://w
ww.springer.com/us/book/9783642201912).
 Chapter 2
\end_layout

\begin_layout Standard
Owen (2000): [Empirical Likelihood](http://statweb.stanford.edu/~owen/empirical/)
\end_layout

\begin_layout Standard
Nash (2014): [On Best Practice Optimization Methods in R](http://www.jstatsoft.org
/v60/i02/paper) 
\end_layout

\end_body
\end_document
