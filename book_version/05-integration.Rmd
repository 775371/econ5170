---
title: 'Integration'
author: "Zhentao Shi"
date: "March 4, 2017"
bibliography: book.bib
fontsize: 11pt
output:
  pdf_document: default
---

# Numerical Integration

In their mathematical definitions, integration and differentiation involve taking limit.
However, our computer is a finite-precision machine that can handle neither arbitrarily
small nor arbitrarily large numbers. It can only approximate the limiting behavior.
In this lecture, we first briefly talk about numerical differentiation and
integration. Next, we discuss stochastic integration in detail with examples.


## Numerical Methods

Numerical differentiation and integration are fundamental topics and of great
practical importance. However, how the computer works out these operations has
nothing to do with economics or econometrics; it is the content of a numerical analysis course.
Here we quickly go over the numerical methods.
@judd1998numerical (Chapter 7) is a useful textbook for details.


In undergraduate calculus, we have learned the analytic differentiation of many common functions.
However, there are cases in which analytic forms are unavailable or too cumbersome
to program.
For instance, in Newton's method for optimization, in principle we need to
code the first and second derivative of an objective function if it is univariate,
or the $K$-dimensional gradient and  $K\times K$-dimensional Hessian matrix if
the objective function is $K$-variate. Programming up the gradient and
the Hessian manually is a time-consuming and error-prone job.
What is worse, whenever we change the objective function,
which happens often at the experimental stage,
we have to redo the gradient and Hessian. Therefore, it is more efficient to use
numerical differentiation instead of coding up the analytical expressions,
in particular in the trial-and-error stage.

The partial derivative of a multivariate function
$f:R^K \mapsto R$ at a point $x_0 \in R^K$ is
$$\frac{\partial f(x)}{\partial x_k}\bigg|_{x=x_0}=\lim_{\epsilon \to 0}
\frac{f(x_0+\epsilon \cdot e_k) - f(x_0 - \epsilon \cdot e_k)}{2\epsilon},$$
where $e_k = (0,\ldots,0,1,0,\ldots,0)$ is the identifier of the $k$-th coordinate.
 In computer, it a follows the basic definition to evaluate $f(x_0\pm\epsilon \cdot e_k))$ with a small
$\epsilon$. But how small is small? Usually it tries a sequence of $\epsilon$'s until
the numerical derivative is stable. There are also more sophisticated algorithms.

In R, the package `numDeriv` does numerical differentiation, in which

* `grad` for a scalar-valued function;
* `jacobian` for a real-vector-valued function;
* `hessian` for a scalar-valued function;
* `genD` for a real-vector-valued function.

In general, integration is more difficult operation
than differentiation. In R, `integrate` carries out one-dimensional quadrature, and
`adaptIntegrate` in the package `cubature` deals with multi-dimensional quadrature.
You can refer to the documentation for the algorithm behind numerical integrations.

Numerical methods are not panacea. Not all functions are differentiable or integrable.
Before turning to numerical methods, it is always important to understand the behavior
of the function at the first place.
R is weak in symbolic calculation, if not completely useless.
`Mathematica` or `Wolfram Alpha` is a powerful tool for this purpose.


## Stochastic Methods

An alternative to numerical integration is the stochastic method.
The underlying principle of stochastic integration is the law of large numbers.
Let  $\int h(x) d P(x)$ be an integral where $P(x)$ is a probability distribution.
We can approximate the integral by
$\int h(x) d P(x) \approx S^{-1} \sum_{s=1}^S h(x_s)$, where $x_s$ randomly
generated from $P(x)$.
When $S$ is large, we can invoke a law of large numbers so that
$$S^{-1} \sum_{s=1}^S h(x_s) \stackrel{\mathrm{p}}{\to} E[h(x)] = \int h(x) d P(x).$$
If the integration is carried out not in the entire support of $P(x)$ by a subset $A$, then
$$\int_A h(x) d P(x) \approx S^{-1} \sum_{s=1}^S h(x_s) \cdot 1\{x_s \in A\},$$
where $1\{\cdot\}$ is the indicator function.

In theory, we want to use a very large $S$. However, we are constrained by computing time
and power. There is no clear guidance of the size of $S$ in practice. Preliminary experiment can
be helpful to decide an $S$ that produces stable results.

Stochastic integration is popular in econometrics and statistics, because of its
convenience in execution.

**Example**

Structural econometric estimation starts from economic principles. There
elements unobservable to the econometric that drive an economic agent's decision.
@roy1951some proposes such a structural model with latent variables, and now is
the foundation of self-selection in labor economics.


In the classical Roy model, a man can assume one of the two occupations: a farmer or a fisher.
The utility of being a farmer is $U_1^{*} = x' \beta_1 + e_1$ and that of
being a fisher is $U_2^{*} = x' \beta_2 + e_2$,
where $U_1^{*}$ and $U_2^{*}$ are latent (unobservable). I
n reality, we observe the binary outcome $y=\mathbf{1}\{U_1^{*}> U_2^{*}\}$. If
$(e_1,e_2)$ is independent of $x$, and
$$
\begin{bmatrix}
e_1\\e_2
\end{bmatrix}
\sim N \left(
\begin{bmatrix}
0 \\ 0
\end{bmatrix},
  \begin{bmatrix}
  \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & 1
  \end{bmatrix}\right)$$
where $\sigma_2^2$ is normalized to be 1, we can write down the log-likelihood of an observable sample as
$$L(\theta) = \sum_{i = 1}^n  \left\{ y_i \log P( U_{i1}^* > U_{i2}^* )
+ (1-y_i)\log P( U_{i1}^* \leq  U_{i2}^* ) \right\}.$$

Let $\theta = (\beta_1, \beta_2, \sigma_1, \sigma_{12}).$ Given a trial value $\theta$,
we can compute
$$p(\theta|x_i) = P\left( U^*_{i1}(\theta) > U^*_{i2}(\theta) \right)
= P\left( x_i'(\beta_1 - \beta_2) > e_{i2} -e_{i1} \right).$$
Under the joint normal assumption, $e_{i2} - e_{i1} \sim N(\sigma_1^2 - 2\sigma_{12}+1).$
Here the analytical form can be easily computed and we do not need simulation.

However, notice that the analytical form depends on the joint normal assumption and cannot be
easily extended to other distributions. As long as the joint distribution of $e_{i1}, e_{i2}$
can be generated from the computer, we can use the stochastic method---the simulation approach.
We estimate
$$
\hat{p}(\theta|x_i) = \frac{1}{S} \sum_{i=1}^S \mathbf{1}\left( U^{s*}_{i1}(\theta) > U^{s*}_{i2}(\theta) \right),
$$
where $s=1,\ldots,S$ is the index of simulation, and $S$ is the total number of simulation replications.

### Generating Random Variables

If the CDF $F(X)$ is known and $U\sim \mathrm{Uniform}(0,1)$, then $F^{-1}(U)$ follows the distribution $F(X)$.

If the pdf $f(X)$ is known, we can generate a sample with such a distribution by the importance sampling.
Metropolis-Hastings algorithm is such a method.
R package `mcmc` implements Metropolis-Hastings algorithm.

**Example**

use [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) to generate a sample of normally distributed observations.
```{r}
library(mcmc)
h = function(x){ y = -x^2 / 2 } # the log, unnormalized function

out = metrop( obj = h, initial = 0, nbatch = 100, nspac = 1  )
plot(out$batch, type = "l") # a time series with flat steps

out = metrop( obj = h, initial = 0, nbatch = 100, nspac = 10  )
plot(out$batch, type = "l") # a time series looks like a white noise

out = metrop( obj = h, initial = 0, nbatch = 10000, nspac = 10  )
summary(out)
plot(density(out$batch))
```


### Optimization

[Expectation-Maximization algorithm](http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) is an iterative estimation method for statistical models in which likelihood is difficult to express in explicit forms.

@arcidiacono2003finite

It is better to categorize the EM algorithm as a numerical optimization method assisted by simulation.

### Indirect Inference

@gourieroux1993indirect

* Economic structural model
* Reduced-form: (auxiliary model)
* Binding function: a mapping from the parameter space of the reduced-form to that
of the structural form.

**Example**

In the Roy model example, the structural parameter is $\theta = (\beta, \sigma_1^2, \sigma_{12} )$. The choice of the reduced-form model is not unique.
A sensible reduced-form model is the linear regression between $y_i$ and $x_i$.
A set of reduced-form parameters can be chosen as
\begin{eqnarray}
b_1 & = & (X'X)^{-1}X'y \\
b_2 & = & n^{-1}\sum_{y_i=1} (y_i - x_i' b_1)^2 = n^{-1}\sum_{y_i=1} (1 - x_i' b_1)^2  \\
b_2 & = & n^{-1}\sum_{y_i=0} (y_i - x_i' b_1)^2 = n^{-1}\sum_{y_i=0} (x_i' b_1)^2
\end{eqnarray}
where in the binding function $b_1$ is associated with $\beta$, and $(b_2,b_3)$ are associated with $(\sigma_1^2,\sigma_{12})$.

**Example**

linear IV model.

### Simulated Method of Moments

@pakes1989simulation: Simulation and the asymptotics of optimization estimators

Match moments generated the theoretical model with their empirical counterparts. The choice of the moments to be matched is not unique either. A set of valid choice is

\begin{eqnarray}
n^{-1} \sum_{i=1}^n x_i (y_i - \hat{p}(\theta | x_i)) & \approx & 0\\
n^{-1} \sum_{i=1}^n (y_i - \bar{y})^2 - \bar{\hat{p}}(\theta) (1- \bar{\hat{p}}(\theta)) & \approx & 0\\
n^{-1} \sum_{i=1}^n (x_i - \bar{x} ) (y_i - \hat{p}(\theta | x_i))^2 & \approx &  0
\end{eqnarray}

where $\bar{y} = n^{-1} \sum_{i=1}^n y_i$ and
$\bar{\hat{p}}(\theta) = n^{-1} \sum_{i=1}^n p(\theta|x_i)$.
The first set of moments is justified by the independence of $(e_{i1}, e_{i2})$ and $x_i$ so that $E[x_i y_i] = x_i E[y_i | x_i] = x_i p(\theta|x_i)$, and the second set matches the variance of $y_i$.
Moreover, we need to choose a weighting matrix $W$ to form a quadratic criterion for GMM.




### Markov Chain Monte Carlo

### Laplace-type estimator

@chernozhukov2003mcmc: An MCMC approach to classical estimation.


If we know the distribution of an extremum estimator, then *asymptotically* the point estimator equals its mean under the quadratic loss function, and equals its median under the absolute-value loss function.

The *Laplace-type estimator* (LTE) transforms the value of the criterion function of an extremum estimator into a probability weight. In a minimization problem, the smaller is the value of the criterion function, the larger it weighs.

**Example**

LTE estimation for linear regression

```{r,eval=FALSE}
library(mcmc)

# DGP
n = 100
b0 = c(1,2)
X = cbind(1, rnorm(n))
Y = X %*% b0 + rnorm(n)

# Laplace-type estimator
L = function(b) -sum( (Y - X %*% b )^2 )  # criterion function

out = metrop( obj = L, initial = c(0,0), nbatch = 10000, nspac = 10  )

# summarize the estimation
bhat2 = out$batch[,2]
bhat2_point = mean(bhat2)
bhat2_var   = var(bhat2)
bhat2_CI = quantile(bhat2, c(.025, .975) )

# compare with OLS
b_OLS = summary( lm(Y~-1+X) )
```

# References
