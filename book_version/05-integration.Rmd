---
title: 'Integration'
author: "Zhentao Shi"
date: "March 4, 2017"
output:
  html_document: default
  pdf_document: default
---



# Numerical Integrations

Integration and differentiation involve taking limit in their mathematical definitions. However, a computer is a finite-precision machine that can handle neither arbitrarily small nor arbitrarily big numbers. We attempt to bridge the gap by approximation.

In this lecture, we will briefly talk about deterministic differentiation and
 integration. However, how the computer works out these operations does not
 have anything to do with econometrics. It is a topic of numerical analysis.




## Derivative

Before we talk about integration, we first briefly talk about derivative.

In undergraduate calculus, we have learned the analytic derivative of many common functions.
However, there are cases that analytic forms are unavailable, or too cumbersome
to code. For example, in Newton's method for optimization, in theory we need to
code the first and second derivative of an objective function if it is univariate,
or the $K$-dimensional gradient and  $K\times K$-dimensional Hessian matrix if
the objective function is $K$-variate. Manually program up the gradient and
the Hessian is a time-consuming job, and it is error-prone.
What is worse, at the experimental stage if the objective function is somehow changed,
then the gradient and Hessian are changed as well. Therefore, we might prefer not
to program up those analytical expressions in the trial-and-error stage.

By definition, the partial derivative of a multivariate function
$f:R^L \mapsto R^K$ at a point $x_0 \in R^L$ is
$$\frac{\partial f(x)}{\partial x}\bigg|_{x=x_0}=\lim_{\epsilon \to 0}
\frac{f(x_0+\epsilon) - f(x_0 - \epsilon)}{2\epsilon}$$
In computer, it a follows the principle to evaluate $f(x_0\pm\epsilon)$ with a small
$\epsilon$. But how small is small? Usually it tries a sequence of $\epsilon$'s until
the numerical derivative is stable. The algorithm can usually be found
in the documentation of the numerical derivative functions.

In R, the package `numDeriv` does numerical derivative. In particular,

* `grad` for a scalar-valued function
* `jacobian` for a real-vector-valued function
* `hessian` for a scalar-valued function
* `genD` for a real-vector-valued function


## Deterministic Numerical Integration

R can also handle numerical integration. In general, integration is more difficult
than differentiation. `integrate` carries out one-dimensional quadrature, and
`adaptIntegrate` in the package `cubature` deals with multi-dimensional quadrature.
You can refer to the documentation for the algorithm behind numerical integrations.
A useful textbook that goes into detail is [Judd].

## Simulated Methods

Only for some special cases we known the analytic form of integration. In many
cases integration is too complex to write in a closed-form. In econometric models,
we often introduce some latent variables. In estimation, we usually integrate out
the latent variables since we can only deal with observable variables. Such
integration is often too complex to be tractable. Numerical integration is a
natural alternative when a closed-form is unavailable.

Another important advantage is that the simulated method makes the implementation
more straightforward, because it closely follows the logic. This is like a direct
proof is often preferred to a contrapositive proof.


**Example**

Roy model is a structural model with latent variables. It describes a mechanism that
leads to observable outcomes.
In the classical Roy model, a man can assume one of the two occupations: a farmer or a fisher. The utility of being a farmer is
$$U_1^{*} = x' \beta_1 + e_1$$
 and that of being a fisher is
 $$U_2^{*} = x' \beta_2 + e_2,$$
 where $U_1^{*}$
and $U_2^{*}$ are latent (unobservable). In reality, we observe the binary outcome $y=\mathbf{1}\{U_1^{*}> U_2^{*}\}$. If
$$
\begin{bmatrix}
e_1\\e_2t
\end{bmatrix}
\sim N \left(
\begin{bmatrix}
0 \\ 0
\end{bmatrix},
  \begin{bmatrix}
  \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & 1
  \end{bmatrix}\right)$$
where $\sigma_2^2$ is normalized to be 1, we can write down the log-likelihood of an observable sample as
$$L(\theta) = \sum_{i = 1}^n  \left\{ y_i \log P( U_{i1}^* > U_{i2}^* )
+ (1-y_i)\log P( U_{i1}^* \leq  U_{i2}^* ) \right\}.$$
The analytical form is complicated because of the presence of correlation in the joint distribution of $(e_1, e_2)$. What is worse, analytical form may not be obtainable if $(e_1, e_2)$ follows some other distribution.

The key difficulty of the analytical approach lies in the explicit form of
$$p(\theta|x_i) = P\left( U^*_{i1}(\theta) > U^*_{i2}(\theta) \right)
= P\left( x_i'(\beta_1 - \beta_2) + e_{i1} - e_{i2} > 0 \right), $$
which is complicated because of the correlation between $e_1$ and $e_2$.
However, given a trial value $\theta$, it is easy to simulate the probability by drawing artificial $(e^s_1, e^s_2)$ from a proposed distribution. In such a simulation approach, we estimate
$$
\hat{p}(\theta|x_i) = \frac{1}{S} \sum_{i=1}^S \mathbf{1}\left( U^{s*}_{i1}(\theta) > U^{s*}_{i2}(\theta) \right),
$$
where $s=1,\ldots,S$ is the index of simulation, and $S$ is the total number of simulation replications.

### Generate Random Variables

If the CDF $F(X)$ is known and $U\sim \mathrm{Uniform}(0,1)$, then $F^{-1}(U)$ follows the distribution $F(X)$.

If the pdf $f(X)$ is known, we can generate a sample with such a distribution by the importance sampling.
Metropolis-Hastings algorithm is such a method.
R package `mcmc` implements Metropolis-Hastings algorithm.

The flexibility to generate any distribution given the pdf makes Bayesian analysis
much more accessible. Bayes is conceptually appealing, but because the PC age is
difficult in implementation. Today Bayes procedure is standard in terms of execution.
The difficulty is the large sample justification. Hardcore Bayesian thinks that
all their results is finite sample. However, one cannot deny the possibility of prior
distribution misspecification. The large sample theory for Bayesian analysis is
a way to see the robustness of finite sample misspecification.

**Example**

Use [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) to generate a sample of normally distributed observations.
```{r}
library(mcmc)
h = function(x){ y = -x^2 / 2 } # the log, unnormalized function

out = metrop( obj = h, initial = 0, nbatch = 100, nspac = 1  )
plot(out$batch, type = "l") # a time series with flat steps

out = metrop( obj = h, initial = 0, nbatch = 100, nspac = 10  )
plot(out$batch, type = "l") # a time series looks like a white noise

out = metrop( obj = h, initial = 0, nbatch = 10000, nspac = 10  )
summary(out)
plot(density(out$batch))
```


### Optimization

[Expectation-Maximization algorithm](http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) is an iterative estimation method for statistical models in which likelihood is difficult to express in explicit forms.

It is better to categorize the EM algorithm as a numerical optimization method assisted by simulation.

### Indirect Inference

* Economic structural model
* Reduced-form: (auxiliary model)
* Binding function: a mapping from the parameter space of the reduced-form to that
of the structural form.

**Example**

In the Roy model example, the structural parameter is $\theta = (\beta, \sigma_1^2, \sigma_{12} )$. The choice of the reduced-form model is not unique.
A sensible reduced-form model is the linear regression between $y_i$ and $x_i$.
A set of reduced-form parameters can be chosen as
\begin{eqnarray}
b_1 & = & (X'X)^{-1}X'y \\
b_2 & = & n^{-1}\sum_{y_i=1} (y_i - x_i' b_1)^2 = n^{-1}\sum_{y_i=1} (1 - x_i' b_1)^2  \\
b_2 & = & n^{-1}\sum_{y_i=0} (y_i - x_i' b_1)^2 = n^{-1}\sum_{y_i=0} (x_i' b_1)^2
\end{eqnarray}
where in the binding function $b_1$ is associated with $\beta$, and $(b_2,b_3)$ are associated with $(\sigma_1^2,\sigma_{12})$.

**Example**

linear IV model.

### Simulated Method of Moments

Pakes and Pollard (1989): Simulation and the asymptotics of optimization estimators

Match moments generated the theoretical model with their empirical counterparts. The choice of the moments to be matched is not unique either. A set of valid choice is

\begin{eqnarray}
n^{-1} \sum_{i=1}^n x_i (y_i - \hat{p}(\theta | x_i)) & \approx & 0\\
n^{-1} \sum_{i=1}^n (y_i - \bar{y})^2 - \bar{\hat{p}}(\theta) (1- \bar{\hat{p}}(\theta)) & \approx & 0\\
n^{-1} \sum_{i=1}^n (x_i - \bar{x} ) (y_i - \hat{p}(\theta | x_i))^2 & \approx &  0
\end{eqnarray}

where $\bar{y} = n^{-1} \sum_{i=1}^n y_i$ and
$\bar{\hat{p}}(\theta) = n^{-1} \sum_{i=1}^n p(\theta|x_i)$.
The first set of moments is justified by the independence of $(e_{i1}, e_{i2})$ and $x_i$ so that $E[x_i y_i] = x_i E[y_i | x_i] = x_i p(\theta|x_i)$, and the second set matches the variance of $y_i$.
Moreover, we need to choose a weighting matrix $W$ to form a quadratic criterion for GMM.




### Markov Chain Monte Carlo

### Laplace-type estimator

Chernozhukov and Hong (2003): An MCMC approach to classical estimation


If we know the distribution of an extremum estimator, then *asymptotically* the point estimator equals its mean under the quadratic loss function, and equals its median under the absolute-value loss function.

The *Laplace-type estimator* (LTE) transforms the value of the criterion function of an extremum estimator into a probability weight. In a minimization problem, the smaller is the value of the criterion function, the larger it weighs.

**Example**

LTE estimation for linear regression

```{r,eval=FALSE}
library(mcmc)

# DGP
n = 100
b0 = c(1,2)
X = cbind(1, rnorm(n))
Y = X %*% b0 + rnorm(n)

# Laplace-type estimator
L = function(b) -sum( (Y - X %*% b )^2 )  # criterion function

out = metrop( obj = L, initial = c(0,0), nbatch = 10000, nspac = 10  )

# summarize the estimation
bhat2 = out$batch[,2]
bhat2_point = mean(bhat2)
bhat2_var   = var(bhat2)
bhat2_CI = quantile(bhat2, c(.025, .975) )

# compare with OLS
b_OLS = summary( lm(Y~-1+X) )
```





## Extended Readings

* Arcidiacono and Jones (2003): [Finite Mixture Distributions, Sequential Likelihood and the EM Algorithm](http://www.jstor.org/stable/1555527?seq=1#page_scan_tab_contents)
* Gourieroux, Monfort and Renault (1993): [Infirect Inference](http://onlinelibrary.wiley.com/doi/10.1002/jae.3950080507/abstract)
