---
output: html_document
title: "Lecture 5: Numerical Optimization"
author: Zhentao Shi
date: March 21, 2016
---

```{r,message=FALSE,warning=FALSE}
library(AER)
library(nloptr)
library(numDeriv)
library(optimx)
```

# Numerical Optimization

## Introduction

Optimization is the key step to apply econometric extremum estimators. A general optimization problem is formulated as
$$\min_{\theta \in \Theta } f(\theta) \mbox{     s.t. }  g(\theta) = 0, h(\theta) \leq 0,$$
where $f(\cdot)$ is a criterion function, $g(\theta) = 0$ is an equality constraint, and $h(\theta)\leq 0$ is an inequality constraint.

Up to now, most established numerical optimization algorithm can a local minimum if it exists. However, there is no guarantee to locate the global minimum when multiple local minima exist.


* unconstrained or constrained

Popular algorithms

* Newton-type algorithm
    - gradient
    - Hessian
* Quasi-Newton type algorithm
    - [BFGS](http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)
* [Nelder-Mead](http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)

R's optimization infrastructure has been improving. [R Optimization Task View](http://cran.r-project.org/web/views/Optimization.html) gives a brief survey of the available CRAN packages.

Recently, the package [optimx](http://cran.r-project.org/web/packages/optimx/index.html) effectively replaces R's default optimization commands. `optimx` delivers a unified interface for various widely-used optimization algorithms. Moreover, it facilitates comparison amongst optimization routines.

**Example**

Pseudo Poisson maximum likelihood

If $y_i$ is a continuous random variable, it obviously does not follow a possion distribution, whose support is non-negative integers. However, if the conditional mean model
$$E[y_i | x_i] = \exp( x_i' \beta),$$
is stastified, we can still use the possion regression to obtain a consistent estimator of the parameter $\beta$ even if $y_i$ does not follow a conditional poisson distribution,

To implement optimization in `R`, it is recommended to write the criterion as a function of the parameter. Data can be provided inside or outside of the function. If the data is fed as additional arguments, these arguments must be explicit.




```{r,cache=TRUE,tidy=TRUE}
# Poisson likelihood
poisson.loglik = function( b ) {
  b = as.matrix( b )
  lambda =  exp( X %*% b )
  ell = -sum( -lambda + y *  log(lambda) )
  return(ell)
}

## prepare the data
data("RecreationDemand")
y =  RecreationDemand$trips
X =  with(RecreationDemand, cbind(1, income) )

## estimation
b.init =  c(0,1)  # initial value
b.hat = optimx( b.init, poisson.loglik, method = c("BFGS", "Nelder-Mead"),
                 control = list(reltol = 1e-7, abstol = 1e-7)  )
print( b.hat )
```




Check `value` in the outcomes for the two algorithms.

In practice no algorithm suits all problems. **Monte Carlo simulation**, where the true parameter is known,  is helpful to check the accuracy of one's optimization routine before applying to an empirical problem, where the true parameter is unknown.

Contour plot visualizes the function surface in a low dimension.

```{r, cache=TRUE, tidy=TRUE}
## contour plot
x.grid = seq(0, 1.8, 0.02)
x.length = length(x.grid)
y.grid = seq(-.5, .2, 0.01)
y.length = length(y.grid)

z.contour = matrix(0, nrow = x.length, ncol = y.length)

for (i in 1:x.length){
  for (j in 1:y.length){
    z.contour[i,j] = poisson.loglik( c( x.grid[i], y.grid[j] )  )
  }
}

contour( x.grid,  y.grid, z.contour, 20)
```

For problems that demand more accuracy, standalone solvers can be invoked via interfaces to R. For example, we can access [NLopt](http://ab-initio.mit.edu/wiki/index.php/NLopt_Installation) through [nloptr](http://cran.r-project.org/web/packages/nloptr/index.html). However, standalone solvers usually have to be compiled and configured. These steps are often not as straightforward as installing most of Windows applications.

NLopt provides an [extensive list of algorithms](http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms#SLSQP).


```{r, cache=TRUE, tidy=TRUE}
## optimization with NLoptr

opts = list("algorithm"="NLOPT_LN_NELDERMEAD",
            "xtol_rel"=1.0e-7,
            maxeval = 500
)

res_NM = nloptr( x0=b.init,
                 eval_f=poisson.loglik,
                 opts=opts)
print( res_NM )

## "SLSQP" is indeed the BFGS algorithm in NLopt, though "BFGS" doesn't appear in the name
opts = list("algorithm"="NLOPT_LD_SLSQP","xtol_rel"=1.0e-7)

poisson.loglik.grad = function( b ) {
  b = as.matrix( b )
  lambda =  exp( X %*% b )
  ell = -colSums( -as.vector(lambda) * X + y *  X )
  return(ell)
}

# check the numerical gradient and the analytical gradient
b = c(0,.5)
grad(poisson.loglik, b)
poisson.loglik.grad(b)


res_BFGS = nloptr( x0=b.init,
                   eval_f=poisson.loglik,
                   eval_grad_f = poisson.loglik.grad,
                   opts=opts)
print( res_BFGS )
```

## Contrained optimization in R

* `optimx` can handle simple box-constrained problems.
* `constrOptim` can handle linear constrained problems.
* Some algorithms in `nloptr`, for example, `NLOPT_LD_SLSQP`, can handle nonlinear constrained problems.
* `Rdonlp2` is an alternative for general nonlinear constrained problems. Rdonlp2 is a package offered by `Rmetric` project. It can be installed by `install.packages("Rdonlp2", repos="http://R-Forge.R-project.org")`

## Convex optimization

If a function is convex in its argument, then a local minimum is a global minimum. Convex optimization is particularly important in high-dimensional problems.

**Example**

* linear regression model MLE
* Lasso (Su, Shi and Phillips, 2015)
* (relaxed) empirical likelihood (Shi, 2015)

[Rmosek](http://rmosek.r-forge.r-project.org/) is an interface in R to access Mosek, a high-quality commercial solver dedicated to convex optimization. Mosek provides free academic licenses. (Rtools is a prerequisite to install Rmosek.)

```{r, eval=FALSE, tidy=TRUE}
require(Rmosek)
lo1 <- list()
lo1$sense <- "max"
lo1$c <- c(3,1,5,1)
lo1$A <- Matrix(c(3,1,2,0,
                  2,1,3,1,
                  0,2,0,3), nrow=3, byrow=TRUE, sparse=TRUE)
lo1$bc <- rbind(blc = c(30,15,-Inf),
                buc = c(30,Inf,25))
lo1$bx <- rbind(blx = c(0,0,0,0),
                bux = c(Inf,10,Inf,Inf))
r <- mosek(lo1)
```

## Extended Readings

* Boyd and Vandenberghe (2004): [Convex Optimization](http://stanford.edu/~boyd/cvxbook/)
* Buhlmann and van de Geer (2011): [Statistics for High-Dimensional Data](http://www.springer.com/us/book/9783642201912). Chapter 2
* Owen (2000): [Empirical Likelihood](http://statweb.stanford.edu/~owen/empirical/)
* Nash (2014): [On Best Practice Optimization Methods in R](http://www.jstatsoft.org/v60/i02/paper)
