---
title: 'Lecture 6: Machine Learning'
author: "Zhentao Shi"
date: "March 20, 2017"
bibliography: book.bib
biblio-style: apalike
link-citations: yes
fontsize: 11pt
output:
  pdf_document: default
---


# Machine Learning


Machine learning has quickly grown into a big field, with applications in scientific research as well as daily life.
The idea in machine learning is general enough to find its value in economics.
From the view of an econometrician, machine learning is a set of data fitting procedures that focus on
out-of-sample prediction.
The simplest illustration is in the regression context.
We repeat a scientific experiment for $n$ times, which generates a dataset $(y_i, x_i)_{i=1}^n$.
What would be the best way to predict $y_{n+1}$ from the same experiment if we observe $x_{n+1}$?

Machine learning is a paradigm shift in econometrics. Conventionally, when an econometrician propose a
new estimator, there are three desirable properties that we pursue one after another.
We first establish its consistency, which is widely seen as the bottom line.
Given a consistent estimator, we want to check whether it is asymptotic normal. Asymptotic normality
is desirable as it holds for many regular estimators and in practice the inferential procedure is
easy to understand and implement. Furthermore, for an asymptotically normal estimator, we want to
show its efficiency, that is, it is the one with the smallest asymptotic variance in a class of
asymptotically normal estimators. This is an optimality property.

Machine learning significantly deviates from the routine. Machine learning researchers dismiss
all the three points. First, because the dataset itself is big enough so that the variance is always small.
They argue efficiency is not crucial. Second, in many context statistical inference the ultimate goal, so
inferential procedure is set aside. For example, the recommendation system on Amazon or Taobao has
a machine learning algorithm behind it. Here we care about the prediction accuracy, not the causal link
why a consumer interested in one good is likely to purchase the other. Third, the world is so complex
that we do not know how the data is generated. We do not have to assume a data generating process (DGP).
If there is no DGP, we lose the standing ground to talk about consistency. Where would my estimator converge
to? With these arguments, the paradigm of conventional econometrics, or statistics in general, is
smashed.

The above arguments have merit, but its also misleading in that it completely rejects the structural
modeling tradition (the Cowles approach). In this lecture, we set the ongoing philosophical debate aside,
and introduce some popular methods that have found applications in economics.


## Variable Selection and Prediction

In modern scientific analysis, the number of covariates $x_i$ can be enormous.
In DNA microarray analysis, we are looking for the association of a symptom to genes.
Variable selection is crucial here because we have scientific evidence that only a small handful of genes
are involved.

Back to economic analysis, the problem of variable selection is not well appreciated, though applied
economists are selecting variables implicitly. They rely on their prior knowledge to choose some
relevant variables from a large number of potential candidates.
For example, the [UK Living Costs and Food Survey](https://discover.ukdataservice.ac.uk/series/?sn=2000028)
has been widely used for analysis of demand theory and family consumption.
A questionnaire in this survey consists of thousand of questions. It is not clear what are the most relevant
variables in general.
In macroeconomics,  @stock2012generalized use 143 US macroeconomic indicators for forecasting exercise.


The most well-known variable selection method in a regression is the least-absolute-shrinkage-and-selection-operator
(Lasso) [@tibshirani1996regression].
Upon on the usual OLS criterion function, Lasso penalizes the $L_1$ norm of the coefficients.
$$
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \lambda \Vert \beta \Vert_1
$$
where $\lambda \geq 0$ is a tuning parameter. In a wide range of values of $\lambda$,
Lasso can shrink some coefficients exact to 0, which suggests that these variables are likely to be
irrelevant in the regression. However, later research finds that Lasso cannot consistently select
the relevant variables from the irrelevant ones.


Another successful variable selection is smoothly-clipped-absolute-deviation (SCAD)
[@fan2001variable]. Their criterion function is
$$
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \sum_{j=1}^d \rho_{\lambda}( |\beta_j| )
$$
where
$$
\rho_{\lambda} (\theta) = \lambda \left\{ 1\{\theta\leq \lambda \} +
\frac{(a\lambda - \theta)_+}{(a-1)\lambda} \cdot 1 \{\theta > \lambda\} \right\}
$$
for some $a>2$ and $\theta>0$. This is a non-convex function, and @fan2001variable establish the so-called
*oracle property*, which means that an estimator can achieve variable selection consistency and
(pointwise) asymptotic normality simultaneously.






The follow-up *adaptive Lasso* [@zou2006adaptive] enjoys selection consistency. Adaptive Lasso is a two
step scheme: 1. First run a Lasso or ridge regression and save the estimator $\hat{\beta}^{(1)}$. 2. Solve
$$
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \lambda \sum_{j=1}^d  w_j |\beta_j|
$$
where $w_j = 1 \bigg/ \left|\hat{\beta}_j^{(1)}\right|^a$ and $a\geq 1$ is a constant. (Common choice is $a = 2$).


In R,  `glmnet` or `LARS` implements Lasso, and `ncvreg` does SCAD. We can set the adaptive Lasso
weight via the argument `penalty.factor` in `glmnet`.


If the purpose of the regression is not variable selection, but for predicition of the response
variables, then there are other methods available.
Mean squared errors (MSE) is a popular criterion for comparison when the response variable is continuous.

An intuitive one is called *stagewise forward selection*.
We start from an empty model. Given many candidate $x_j$, in each round we add the regressor that can
produce the biggest $R^2$. This method is similar to the idea of $L_2$ componentwise boosting,
which does not adjust the coefficients fitted earlier.


To be discussed in the future

* The idea of boosting and its association to reweighting.
* regression tree, bagging (Killian and Inoue), average of subsampling



## References
