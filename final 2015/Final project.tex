% Date: 9/7/2014
% Proposal with a formal model

\documentclass[12pt,pdftex,letterpaper]{article}

% set the text parameters
            \setlength{\textwidth}{6.5in}
            \setlength{\textheight}{8.5in}
            \addtolength{\topmargin}{-\topmargin}
            \setlength{\oddsidemargin}{0in}
            \setlength{\evensidemargin}{0in}
% set the space
\usepackage[doublespacing]{setspace}

% type out equations and math symbols
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lipsum}
\usepackage{forarray}
\usepackage{amstext}

\usepackage{amsfonts}


\usepackage{float}
\usepackage{cases}

\usepackage{longtable,lscape}
\usepackage{rotating,threeparttable}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{pgf,tikz}
\usepackage{color}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{array}
\usepackage{ragged2e}
\usepackage{footnote}
\usepackage{hyperref}
\usepackage{subfigure}



\makeatletter
    \newtheorem{proposition}{\translate{Proposition}}
    % Compatibility
    \newenvironment{Lemma}{\begin{lemma}}{\end{lemma}}
    \newenvironment{Theorem}{\begin{theorem}}{\end{theorem}}
    \newenvironment{Proposition}{\begin{proposition}}{\end{proposition}}
    \newenvironment{Corollary}{\begin{corollary}}{\end{corollary}}
    \newenvironment{Example}{\begin{example}}{\end{example}}
    \newenvironment{Examples}{\begin{examples}}{\end{examples}}
    \newenvironment{Definition}{\begin{definition}}{\end{definition}}
\makeatother


\title{\large LIML and LIML-LASSO Project}
\date{\today}
\author{1155040608 Yichuan Hu\footnote{Department of Economics, The Chinese University of Hong Kong.  {\bf E-mail}
\href{mailto:huyichuan175@gmail.com}{huyichuan175@gmail.com}} \\
1155040614 Yufeng Sun\footnote{Department of Economics, The Chinese University of Hong Kong. {\bf E-mail}\href{mailto: pkuskyfree@gmail.com}{ pkuskyfree@gmail.com}}\\
1155040750 Jing Zhao\footnote{Department of Economics, The Chinese University of Hong Kong. {\bf E-mail}\href{mailto: jing.zhao750@gmail.com}{ jing.zhao750@gmail.com}}
}

\begin{document}
\maketitle
\begin{center}
\vspace{3mm}
\end{center}


     The project explores the performance of LIML and LIML-LASSO. This report is organized as below. Section 1 shows the key problem of this project; section 2 discusses the algorithm of LIML and LIML-LASSO; section 3 lists the simulation results, conducts sensitive analysis and suggests an alternative transformation algorithm; section 4 summarizes the key results of this project.
     
\section{The Problem}   
\indent   This entire practice is based on a particular data generating process, following the steps:

   Step 1: $(z_{ij})_{j=1}^{L},(u_{ik})_{k=1}^{K},(\epsilon_{i})_{i=1}^{n}$ are drawn from the standard normal distribution and independent of each other.
   
   Step 2:Using the random variables drawn from step 1, the rest of the data will be generated as below:
   $$y_{i}=b_{1}x_{i1}+\dots+b_{1}x_{iK_{0}}+b_{2}x_{i(K_{0}+1)}+\dots+b_{2}x_{iK}+\epsilon_{i}, i=1 \sim n$$
   $$x_{ij}=1+\gamma_{1}z_{ij}+\gamma_{2}\epsilon_{i}+(1-|\gamma_{1}|-|\gamma_{2}|)u_{ik},j=1\sim K_{0}$$
   $$x_{ij}=1+\gamma_{1}z_{ij}+(1-|\gamma_{1}|-|\gamma_{2}|)u_{ik},j=K_{0}+1 \sim K$$
   
   There are 4 different settings of parameters.
   
   Case 1: $b_{1}=1,b_{2}=0,\gamma_{1}=\gamma_{2}=0.3,L=1.1K$
   
   Case 2: $b_{1}=1,b_{2}=0.01,\gamma_{1}=\gamma_{2}=0.3,L=1.1K$
   
   Case 3: $b_{1}=-1,b_{2}=0,\gamma_{1}=\gamma_{2}=0.3,L=1.1K$
   
   Case 4: $b_{1}=-1,b_{2}=0.01,\gamma_{1}=\gamma_{2}=0.3,L=1.1K$
   
   The above DGP implies the underlying structural links of the 3 sets of variables. It describes a special situation where the number of endogenous variable is relatively large, leading to the problem of ``too many parameters". Moreover, many of these coefficients are zero or close to zero, which is referred to as the ``sparsity" problem. More specifically, the endogenous variables are
      $$y_{i},(x_{ij})_{j=1}^{K_{0}},$$
and the exogenous variables are
      $$(x_{ij})_{j=K_{0}+1}^{K},(z_{ij})_{j=1}^{L}.$$
      
    The underlying structural model can be written as
    $$y_{i}=\tilde{b}_{1}x_{i1}+\dots+\tilde{b}_{K_{0}}x_{iK_{0}}+\tilde{b}_{K_{0}+1}x_{i(K_{0}+1)}+\dots+\tilde{b}_{K}x_{iK}+\epsilon_{i}, i=1 \sim n$$
    $$x_{ij}=1+\sum_{j=1}^{K_{0}}\tilde{\gamma}_{ij}z_{ij}+\tilde{\gamma}_{2}\epsilon_{i}+(1-|\tilde{\gamma}_{1}|-|\tilde{\gamma}_{2}|)u_{ik},j=1\sim K_{0}$$
    $$x_{ij}=1+\sum_{j=K_{0}+1}^{K}\tilde{\gamma}_{ij}z_{ij}+(1-|\tilde{\gamma}_{1}|-|\tilde{\gamma}_{2}|)u_{ik},j=K_{0}+1 \sim K$$ 
    
    Now it's clear that the first objective of this project is to estimate the coefficients $(\tilde{b}_{j})_{j=1}^{K},(\tilde{\gamma}_{j1},\tilde{\gamma}_{j2})_{j=1}^{K}$ based on the artificial data $(y_{i},x_{ij},z_{ij})_{i=1,j=1}^{n,K}$.
    
    Theoretically, several candidate estimation methods can provide reasonable results. The next task is to evaluate their performances by comparing their bias and variance. Finally, the robustness of the conclusions should be checked with different combinations of coefficients, based on the parameter settings of 4 cases mentioned in DGP.

      
\section{LIML}
\subsection{Iteration algorithm}
\begin{align*}
y-\beta^{'}X & =\epsilon\\
X-\Gamma Z & = V 
\end{align*}

\begin{equation}
\begin{bmatrix}
 1 & -\beta^{'} \\
 0  &     I  \\
\end{bmatrix}
\begin{bmatrix}
y_{1} \\
X
\end{bmatrix}
+
\begin{bmatrix}
0 \\
-\Gamma 
\end{bmatrix}
Z
=
\begin{bmatrix}
\epsilon \\
V
\end{bmatrix}
\end{equation}

Let :
\begin{equation*}
B=\begin{bmatrix}
 1 & -\beta^{'} \\
 0  &     I  \\
\end{bmatrix}
,
Y=
\begin{bmatrix}
y_{1} \\
X
\end{bmatrix}
,
\tilde{\Gamma}=
\begin{bmatrix}
0 \\
-\Gamma 
\end{bmatrix}
\end{equation*}

and $\Sigma$ is the variance-covariance matrix of the  
$\begin{bmatrix}
\epsilon \\
V
\end{bmatrix}$

The likelihood function is:
$$Q_{n}=-\frac{K_{0}}{2}\mbox{log}(2\pi)-\frac{1}{2}\mbox{log}(|\Sigma(\sigma)|)-\frac{1}{2n}\sum_{i=1}^{N}[BY_{i}+\tilde{\Gamma}Z_{i}]'{\Sigma}^{-1}[BY_{i}+\tilde{\Gamma}Z_{i}]$$


Given the likelihood function $Q_{n}$, the iteration will follow below steps:

Step 1: Given a initial value $\Sigma$, calculate the parameters:$(\tilde{b}_{j})_{j=1}^{K},(\tilde{\gamma}_{j1},\tilde{\gamma}_{j2})_{j=1}^{K}$

Step 2: Based on the calculating results of $(\tilde{b}_{j})_{j=1}^{K},(\tilde{\gamma}_{j1},\tilde{\gamma}_{j2})_{j=1}^{K}$, estimate $\Sigma$.

Step 3: Repeat above steps until the results converge.




\subsection{LIML-LASSO-AIC}
Based on the likelihood function generated by equation (1)


$$\min_{\beta_{n}} Q_{n}+\rho_{n}||\beta_{n}||_{1}$$

The AIC criteria function is:
$$J_{B_{n}}^{AIC}(\beta_{n})=\frac{1}{n^2}Q_{n}(\beta_{n})+\frac{2}{n}B_n|\beta_{n}|_{0}$$

where $|\cdot|_{0}$ is the number of non-zero components in a vector.

\leftline{\textbf{Algorithm}}

Step 1: Choose $\rho_{i} \in \{a_{1},a_{2},\dots a_{h} \},i=1 \sim h$ and calculate 
     $$\beta_{ni}=argmin({\frac{1}{n^{2}} Q_{n}(\beta_{n})+\rho_{n}||\beta_{n}||_{1}})$$

Step 2: Based on $\beta_{ni}$ in step 1, calculate $J_{B_{n}}^{AIC}(\beta_{ni})$ and choose:
      $$\beta_{n}^{op}=argmin_{i =1 \sim h}(J_{B_{n}}^{AIC}(\beta_{ni}))$$

Step 3: From $\beta_{n}^{op}$ we can choose the optimal $\rho^{op}=\hat{\rho}_{n}^{(AIC)}$.


\subsection{LIML-LASSO-BIC}
Given the objective function:

$$\min_{\beta_{n}} Q_{n}+\rho_{n}||\beta_{n}||_{1}$$

The BIC criteria function is:
$$J_{B_{n}}^{BIC}(\beta_{n})=\frac{1}{n^2}Q_{n}(\beta_{n})+\frac{log(n)}{n}B_{n}|\beta_{n}|_{0}$$

\leftline{\textbf{Algorithm}}
Step 1:Choose $\rho_{i} \in \{a_{1},a_{2},\dots a_{h}\},i=1 \sim h$ and calculate 
     $$\beta_{ni}=argmin(\frac{1}{n^{2}}Q_{n}(\beta_{n})+\rho_{n}||\beta_{n}||_{1})$$

Step 2:Based on $\beta_{ni}$ in step 1, calculate $J_{B_{n}}^{BIC}(\beta_{ni})$ and choose:
      $$\beta_{n}^{op}=argmin_{i =1 \sim h}(J_{B_{n}}^{BIC}(\beta_{ni}))$$

Step 3:From $\beta_{n}^{op}$ we can choose the optimal $\rho^{op}=\hat{\rho}_{n}^{(BIC)}$.


\section{Analysis of Simulation result}
\subsection{Comparison criteria}
    We will evaluate the performances of different estimators by comparing their MSE. Since MSE can be decomposed into two components related to variance and bias respectively, the trade-off between variance and bias in MSE is of the key interest in the following analysis. 
    
In general:
\begin{align*}
MSE(\hat{\theta})&=E((\hat{\theta}-\theta)^{2})\\
                 &=E[(\hat{\theta}-E(\hat{\theta})+E(\hat{\theta})-\theta)^{2}] \\
                 &=E[(\hat{\theta}-E(\hat{\theta}))^{2}+2(\hat{\theta}-E(\hat{\theta}))(E(\hat{\theta})-\theta)+(E(\hat{\theta})-\theta)^{2}] \\
                 &=E[(\hat{\theta}-E(\hat{\theta}))^{2}]+E[(E(\hat{\theta})-\theta)^{2}] \\
                 &=Var(\hat{\theta})+Bias(\hat{\theta},\theta)^{2} 
\end{align*}
\begin{align*}
\hat{MSE}(\hat{\beta})&=\sum_{k=1}^{K}[\frac{1}{R}\sum_{r=1}^{R}(\hat{\beta}_{rk}-\beta_{k}^{0})^{2}] \\
                      &=\sum_{k=1}^{K}[\frac{1}{R}\sum_{r=1}^{R}(\hat{\beta}_{rk}-\frac{1}{R}\sum_{r=1}^{R}\hat{\beta}_{rk})^{2}]+\sum_{k=1}^{K}(\frac{1}{R}\sum_{r=1}^{R}\hat{\beta}_{rk}-\beta_{k}^{0})^{2}   
\end{align*}

\subsection{Simulation results}
\indent Theory predicts that LIML estimator has small bias, but large variance. We hope to use simulation results to verify this hypothesis. We obtain the estimator in three approaches, closed-form LIML, iterated LIML and LIML-LASSO. Closed-form LIML and iteration LIML are essentically LIML estimator with no penalty. The difference between these two estimators evaluates the performance of the iteration algorithm. The last estimator, LIML-LASSO, has no closed form solution. Therefore, it can only be calculated by the iteration method.

\begin{figure}[h]
\centering
\subfigure{
\begin{minipage}[b]{0.4\textwidth}
\includegraphics[width=1\textwidth]{figure1.jpg} \\
\includegraphics[width=1\textwidth]{figure2.jpg}
\end{minipage}
}
\subfigure{
\begin{minipage}[b]{0.4\textwidth}
\includegraphics[width=1\textwidth]{figure3.jpg} \\
\includegraphics[width=1\textwidth]{figure4.jpg}
\end{minipage}
}
\caption{Baseline $K=20$}\label{k20}
\end{figure}

\indent Figure \ref{k20} is our baseline result. It sets the explanatory variable $K=20$, the endogenous variable $K_0=10$, the sample size $N=200$, and the repeating times 100. The four panels explore the cases when $b_1=1$ or -1, $b_2=0$ or 0.01, respectively. For each panel, only the result of closed-form solution is consistent with our expectation, that the variance of LIML dominates the bias. The iteration method, however, always yields large bias and small variance, which contradicts what the theory predicts. This result remains unchanged when the repeating times increase to 250 (Figure \ref{rep250}), or the explanatory variable $K$ increase to 40 (sample size rise to 400, Figure \ref{k40}).

\begin{figure}[h]
\centering
\subfigure{
\begin{minipage}[b]{0.4\textwidth}
\includegraphics[width=1\textwidth]{figure1-250.jpg} \\
\includegraphics[width=1\textwidth]{figure2-250.jpg}
\end{minipage}
}
\subfigure{
\begin{minipage}[b]{0.4\textwidth}
\includegraphics[width=1\textwidth]{figure3-250.jpg} \\
\includegraphics[width=1\textwidth]{figure4-250.jpg}
\end{minipage}
}
\caption{Repeating times 250}\label{rep250}
\end{figure}

\begin{figure}[h]
\centering
\subfigure{
\begin{minipage}[b]{0.4\textwidth}
\includegraphics[width=1\textwidth]{figure1-40.jpg} \\
\includegraphics[width=1\textwidth]{figure2-40.jpg}
\end{minipage}
}
\subfigure{
\begin{minipage}[b]{0.4\textwidth}
\includegraphics[width=1\textwidth]{figure3-40.jpg} \\
\includegraphics[width=1\textwidth]{figure4-40.jpg}
\end{minipage}
}
\caption{Large dimension $K=40$, sample size $N=400$}\label{k40}
\end{figure}

\indent One possible explanation for this undesirable result is the limited sample size. When $K=20$, the number of parameters to be estimated is 420, way more than the sample size 200. Considering the limited computational speed, we reduce the parameter dimension to $K=2$ and $K_0=1$, rather than increase the sample size directly. The bias-variance tradeoff is displayed in figure \ref{k2}. The closed-form solution yields large variance and small bias. But the iteration method still has huge bias and small variance. The mean of estimated $b_1$ is about 1.2, given the true $b_1=1$. This illustrates that small sample size might not be the main reason for the undesirable result.

\begin{figure}[h]
\centering
\subfigure{
\begin{minipage}[b]{0.4\textwidth}
\includegraphics[width=1\textwidth]{figure5.jpg} \\
\includegraphics[width=1\textwidth]{figure6.jpg}
\end{minipage}
}
\subfigure{
\begin{minipage}[b]{0.4\textwidth}
\includegraphics[width=1\textwidth]{figure7.jpg} \\
\includegraphics[width=1\textwidth]{figure8.jpg}
\end{minipage}
}
\caption{Small dimension $K=2$}\label{k2}
\end{figure}

\indent Another explanation is that the problem arises from implementing the ``CVX" package in Matlab. The optimisation might not be a convex problem when the parameter dimension is large. Hence, the reported estimation might be a local optimum, not a global one. To test this hypothesis, we use R to redo the problem. By invoking the standard ``nlm" package for optimisation, we avoid any blackbox in the entire algorithm. Figure \ref{r} compares the result between two algorithms. R estimation yields the predicted result, that variance of LIML dominates the bias.

\begin{figure}[h]
\centering
\subfigure[CVX]{
\begin{minipage}[b]{0.4\textwidth}
\includegraphics[width=1\textwidth]{figure5.jpg} \\
\includegraphics[width=1\textwidth]{figure6.jpg}
\end{minipage}
}
\subfigure[R estimation]{
\begin{minipage}[b]{0.4\textwidth}
\includegraphics[width=1\textwidth]{figure-r.jpg} \\
\includegraphics[width=1\textwidth]{figurer-2.jpg}
\end{minipage}
}
\caption{CVX and R estimation}\label{r}
\end{figure}

\subsection{Sensitive analysis}
\indent The LIML-LASSO estimation method relies on 2 critical tuning parameters, controlling the weights of penalty ($\rho_{n}$) and the form of penalty respectively. In this part of analysis, a simple experiment is conducted to explore whether the estimation results are sensitive to the weight and form of the penalty function. 

\indent The benchmark model is set as $K=20$, $K_{0}=10$, with 200 observations ($n=200$) and $L_{1}$-norm  penalty function. We will change the settings in 2 ways to see if the estimators change. First, the weight parameter $\rho_{n}$ is chosen from a grid of 23 points ranging from $10^{-6}$ to 10. In addition to changing the weight parameter, we hope to explore the role of penalty function as well. Thus we choose 3 forms of penalty function, namely $L_{1}$-norm, $L_{2}$-norm  and $L_{\infty}$-norm of $\beta$. Combining all these variations, 69 different scenarios are generated. In each scenario, although all the 20 coefficients are estimated, we only report the mean of all large coefficients whose true value are 1 as $b_{1}$, and mean of the small coefficients whose true value are 0 as $b_{2}$. 

\indent The results of the first sensitivity analysis is reported in table \ref{Table 1}.

\begin{table}[h]
\centering
\caption{Sensitivity analysis}
\label{Table 1}
\resizebox{15cm}{!}{
\begin{tabular}{@{}llllllll@{}}
\toprule
\multicolumn{2}{l}{True Value} & \multicolumn{3}{c}{$\beta_{1}=\cdots \beta_{10}=1(b_{1} =1$)} & \multicolumn{3}{c}{$\beta_{11}=\cdots=\beta_{20}(b_{2}=0)$} \\ \midrule
\multicolumn{2}{l}{LIML (no penalty)} & \multicolumn{3}{c}{1.050823675} & \multicolumn{3}{c}{0.034747861} \\
\multicolumn{2}{l}{$\rho_{n}$} & $L_{1}$-norm  & $L_{2}$-norm & $L_{\infty}$-norm & $L_{1}$-norm  & $L_{2}$-norm & $L_{\infty}$-norm \\
$10^{-5}$ & 0.00001 & 1.050583304 & 1.050570167 & 1.050791657 & 0.034645795 & 0.034901391 & 0.034738681 \\
\multirow{3}{*}{$10^{-4}$} & 0.0001 & 1.049088942 & 1.048651978 & 1.050894919 & 0.033620802 & 0.036262457 & 0.034653677 \\
 & 0.0004 & 1.04373751 & 1.042066248 & 1.050934916 & 0.03057689 & 0.040800475 & 0.034571518 \\
 & 0.0007 & 1.038229702 & 1.035532837 & 1.049438259 & 0.027817563 & 0.045301029 & 0.03588704 \\
\multirow{3}{*}{$10^{-3}$} & 0.001 & 1.031809949 & 1.029020952 & 1.047755007 & 0.026285307 & 0.049761348 & 0.037139125 \\
 & 0.004 & 0.96258326 & 0.96786628 & 1.029583504 & 0.017794949 & 0.091094653 & 0.049701566 \\
 & 0.007 & 0.891862333 & 0.914895709 & 1.009084725 & 0.0100368 & 0.126631256 & 0.063510624 \\
\multirow{3}{*}{$10^{-2}$} & 0.01 & 0.817168564 & 0.867246749 & 0.987305358 & 0.005004837 & 0.157275258 & 0.078236795 \\
 & 0.04 & 0.099862869 & 0.568072317 & 0.772524651 & 1.60E-08 & 0.273347964 & 0.239729254 \\
 & 0.07 & 4.29E-09 & 4.03E-01 & 0.583987048 & 2.18E-09 & 0.252911476 & 0.381113851 \\
\multirow{3}{*}{$10^{-1}$} & 0.10000 & 2.94E-09 & 2.74E-01 & 0.500729358 & 1.75E-09 & 1.95E-01 & 0.422492709 \\
 & 0.40000 & 3.57E-10 & 2.72E-09 & 0.263151416 & 3.03E-10 & 1.93E-09 & 0.263151393 \\
 & 0.70000 & 7.29E-11 & 5.44E-10 & 5.16E-02 & 7.12E-11 & 4.19E-10 & 5.16E-02 \\
1 & 1.00000 & 4.04E-11 & 2.95E-10 & 1.09E-08 & 4.14E-11 & 2.48E-10 & 9.14E-09 \\ \bottomrule
\end{tabular}
}
\end{table}

\indent Looking at the table vertically, we can conclude that the estimation results are very sensitive to the values of the weight parameter ($\rho_{n}$) in the range from $10^{-4}$ to $10^{-1}$. When $\rho_{n}$ is too small, less or equal to $10^{-5}$, the LIML-LASSO estimator acts as if there's no penalty at all and will be similar to pure LIML estimator.   When $\rho_{n}$ is larger than 1, however, the estimation will be severely biased as all the estimated coefficients are squeezed to zero. Moreover, as $\rho_{n}$ increases, indicating a heavier penalty on increasing the absolute value of $\beta$, the values of estimators decrease. 

\indent Looking at the table horizontally, the results seem not very sensitive to the form of penalty function.  Moreover, by the AIC /BIC criteria, the optimal LIML-LASSO estimators under each penalty function of $L_{1}$-norm, $L_{2}$-norm and $L_{\infty}$-norm  are $(0.962583, 0.017795)$, $(0.867247, 0.157275)$ and $(0.772525, 0.239729)$ respectively. It seems that with the benchmark setting, $L_{1}$-norm  penalty works best. However, it's obvious that with $L_{2}$-norm and $L_{\infty}$-norm, LIML-LASSO can perform better with alternative $\rho_{n}$ values not chosen by AIC/BIC, for example $(1.029020952,0.049761348)$ with $\rho_{n}=0.001$ and $(1.009084725,0.063510624)$ with $\rho_{n}=0.007$. 

\indent One possible explanation may be that AIC/BIC are not suitable to choose optimal weights for $L_{2}$-norm and $L_{\infty}$-norm penalty. The LIML-LASSO with $L_{1}$ penalty has the power to make some estimated coefficients exactly zero, which will significantly decrease the value of AIC/BIC criteria function.  However,$L_{2}$-norm and $L_{\infty}$ can hardly force the estimated  coefficients to be exactly zero. Instead, with $L_{2}$-norm and $L_{\infty}$-norm penalty, the two setting of parameters, the large ones and the small ones are likely to concentrate around a middle value.  These arguments can be verified by the fact that $L_{1}$-norm always do better in estimating $b_{2}$, the small coefficient set to be zero. In addition, when  $\rho_{n}$ is larger than 0.4, all the estimated $\beta$s become identical.

\indent Therefore, several  implications can be made. First, the composition of the coefficients and the AIC/BIC criteria interfer the performance of LIML-LASSO with $L_{2}$-norm and $L_{\infty}$-norm penalty. Under the benchmark setting, where many of the coefficients are exactly zero, AIC/BIC helps select the best $\rho$ with $L_{1}$-norm penalty and $L_{1}$-norm performs best. When the number of large coefficient increases and the small coefficients are not that close to zero, we  suggest that $L_{\infty}$-norm and $L_{2}$-norm work better. However, to verify the above guess, we need to modify the AIC/BIC criteria first to help us select optimal weights first.

\indent In summary, the weight parameter $\rho_{n}$ is critical to the estimation result, while the penalty function form is less important. Although under the benchmark setting, where a large number of coefficients are exactly zeros, $L_{1}$-norm penalty performs best. Adjustment on penalty function form can be made to improve the performance of LIML-LASSO estimator when the composition of coefficients changes. Finally, AIC/BIC works well for $L_{1}$-norm penalty but not for the others. 

\subsection{Discussion of an alternative iteration solution}
\subsubsection{Idea}
The above discussion is based on the likelihood function generated by the equation (1). There is an alternative method to transfer the equation (1).

   Multiplying the matrix in the two sides of equation (1):
\begin{equation*}
\begin{bmatrix}
1 & 0 \\
0 & -\beta^{'}
\end{bmatrix}
\end{equation*}

will be

\begin{equation*}
\begin{bmatrix}
 1 & -\beta^{'} \\
 0  & -\beta^{'}  \\
\end{bmatrix}
\begin{bmatrix}
y_{1} \\
X
\end{bmatrix}
+
\begin{bmatrix}
0 \\
\beta^{'}\Gamma
\end{bmatrix}
Z
=
\begin{bmatrix}
\epsilon \\
-\beta^{'}V
\end{bmatrix}
\end{equation*}

Let:
\begin{equation*}
\tilde{B}=
\begin{bmatrix}
 1 & -\beta^{'} \\
 0  & -\beta^{'}  \\
\end{bmatrix}
,
\Lambda
=
\begin{bmatrix}
0 \\
\beta^{'}\Gamma
\end{bmatrix}
\end{equation*}

and $\tilde{\Sigma}$ is the variance-covariance matrix of the  
$\begin{bmatrix}
\epsilon \\
-\beta^{'}V
\end{bmatrix}$

The likelihood function is:
$$\tilde{Q}_{n}=-\frac{K_{0}}{2}\mbox{log}(2\pi)-\frac{1}{2}\mbox{log}(|\tilde{\Sigma}(\sigma,\beta)|)-\frac{1}{2n}\sum_{i=1}^{n}[\tilde{B}Y_{i}+\Lambda Z_{i}]'{\tilde{\Sigma}(\beta,\sigma)}^{-1}[\tilde{B}Y_{i}+\Lambda Z_{i}]$$

The iteration will follow the steps:

Step 1: Given a initial value $\tilde{\Sigma}(\sigma,\beta)$, calculate the parameters: $(\tilde{b}_{j})_{j=1}^{K},(\tilde{\gamma}_{j1},\tilde{\gamma}_{j2})_{j=1}^{K}$

Step 2: Based on the calculating results of $(\tilde{b}_{j})_{j=1}^{K},(\tilde{\gamma}_{j1},\tilde{\gamma}_{j2})_{j=1}^{K}$, estimate $\tilde{\Sigma}(\sigma,\beta)$.

Step 3: Repeat the above steps until the results converge.

\subsubsection{Simulation performance}
\begin{table}[h]
\centering
\caption{One simulation result based on the alternative algorithm}
\label{Table 2}
\resizebox{15cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$b_{1}$ & 0.5141 & 0.6244  & 0.4376  & 0.7229  & 0.7628 & 0.6534 & 0.4490 & 0.6605  & 0.5759  & 0.6084 \\
\hline
Ture$b_{1}$ & 1  & 1       & 1       &   1     & 1 & 1 & 1 & 1  & 1  & 1\\\hline
$b_{2}$ & 0.0555 & -0.0737 & -0.0458 & -0.3724 & 0.0914 & 0.4924 & 0.0349 & -0.2134 & -0.0211 &  -0.0016\\\hline 
Ture$b_{2}$ & 0  & 0       & 0       &   0     & 0 & 0 & 0 & 0  & 0  & 0\\\hline    
\end{tabular}
}
\end{table}
The simulation results shown in table \ref{Table 2} based on this transformation algorithm seems bad, compared to the true parameter value. We try to guess the reason in the following part.

\subsubsection{Analysis of the potential problem}
    The significant deviation of the simulation results based on the alternative approach may come from the pitfall of the transformation procedure.
    
    Note that in $Q_{n}$ of the original objective function, $\Sigma(\sigma)$ is only a function of $\sigma$. So we can fix $\Sigma(\sigma)$, calculate the $\beta$ and repeat the process until converge. 
    
    In $\tilde{Q}_{n}$, $\tilde{\Sigma}(\sigma,\beta)$ is a function of both $\sigma$ and $\beta$ (for the existence of $-\beta^{'}V$).If we still fix  $\tilde{\Sigma}(\sigma,\beta)$ and then calculate the optimal $\beta$, there will be a problem in this optimization problem.
    
    Consider a simple example.
    $$\min_{x,y} x^{2}+z(x,y)$$    
    where $z(x,y)=y^{2}-2x.$
    
    If we fix $z(x,y)$, then the optimal $x$ should be $x=0$, then $z(0,y)=y^{2}$ and the optimal $y=0$.
    
    If we use all information about $x$, we will get that optimal $x=1$ and the optimal $y=0$.
    
    So the transformation in the alternative algorithm makes that the variance-covariance become a function of $\beta$. If we still fix $\tilde{\Sigma}(\sigma,\beta)$ in the iteration procedure, this maybe a pitfall of the algorithm.
    
\section{Conclusion}
    In summary, above discussion and simulation results show that:
    
    (1)The variance dominates the LIML MSE while the bias dominates the LIML-LASSO MSE.
    
    (2)The LIML-LASSO estimator is sensitive to the choice of the tuning parameter $\rho_{n}$, controlling the weight of penalty while is not very sensitive to the choice of penalty-function.
    
    (3)The original iteration algorithm based on objective function (1) provides a bias-dominance estimator, while the iteration algorithm based on transformation may suffer from the transformation pitfall.
%\bibliographystyle{AER}
%\bibliography{literature}


\end{document}
