#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand\theenumi{(\alph{enumi})}
\renewcommand\labelenumi{\theenumi}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format dvi
\output_sync 1
\output_sync_macro "\synctex=-1"
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\branch abc
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\branch pf of gamma = 0
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Numerical Algorithm for 
\begin_inset Newline newline
\end_inset

Generalized Empirical Likelihood
\end_layout

\begin_layout Standard
The generalized empirical likelihood can be written as 
\begin_inset Formula 
\[
\min_{\beta,\lambda}\sum_{i=1}^{n}\rho\left(\lambda'g_{i}\left(\beta\right)\right)
\]

\end_inset

where 
\begin_inset Formula $\rho\left(v\right)=-\log\left(1+v\right)$
\end_inset

, 
\begin_inset Formula $\rho\left(v\right)=\exp\left(v\right)$
\end_inset

 and 
\begin_inset Formula $\rho\left(v\right)=v^{2}$
\end_inset

 correspond to the empirical likelihood, exponential tilting, and the continuous
ly updating GMM, respectively.
\end_layout

\begin_layout Standard
The empirical likelihood literature well recognizes the difficulty of numerical
 optimization.
 It was suggested (Owen 2001, Kitamura 2006) that we implement the optimization
 in two steps.
 The inner loop and the outer loop.
 The outer loop is a general nonlinear optimization.
 It may have multiple local minima, while the inner loop is an convex problem.
\end_layout

\begin_layout Standard
I propose in this note one way to reduce the computational burden of outer
 loop function evaluation that requires to solve the inner optimization.
 
\end_layout

\begin_layout Standard
At any initial value 
\begin_inset Formula $\widehat{\beta}_{t-1}$
\end_inset

, there is a corresponding 
\begin_inset Formula $\widehat{\lambda}_{t-1}$
\end_inset

.
 
\begin_inset Formula $\widehat{\beta}_{0}$
\end_inset

 is the initial value.
 We suggest the update direction and the step size.
 Let 
\begin_inset Formula 
\[
\widehat{\beta}_{t}=\arg\min_{\beta}\rho\left(\widehat{\lambda}_{t-1}'g_{i}\left(\widehat{\beta}_{t-1}\right)+\widehat{\lambda}'_{t-1}G_{i}\left(\widehat{\beta}_{t-1}\right)\left(\beta-\widehat{\beta}_{t-1}\right)\right)
\]

\end_inset

where 
\begin_inset Formula $G_{i}\left(\beta\right)=\partial g_{i}\left(\beta\right)/\partial\beta'$
\end_inset

 is the Jacobian.
 The problem is convex in 
\begin_inset Formula $\beta$
\end_inset

, so easy to solve.
\end_layout

\begin_layout Standard
The intuition is to approximate 
\begin_inset Formula $g\left(\widehat{\beta}_{t}\right)$
\end_inset

 by 
\begin_inset Formula $g\left(\beta_{t-1}\right)+G\left(\widehat{\beta}_{t-1}\right)\left(\widehat{\beta}_{t}-\widehat{\beta}_{t-1}\right)$
\end_inset

, a linear approximation.
 After updating 
\begin_inset Formula $\widehat{\beta}_{t-1}$
\end_inset

 by 
\begin_inset Formula $\widehat{\beta}_{t}$
\end_inset

, we then update 
\begin_inset Formula $\widehat{\lambda}_{t-1}$
\end_inset

 by 
\begin_inset Formula $\widehat{\lambda}_{t}$
\end_inset

, which is a convex problem as well.
 Next we start another iteration of the updating procedure.
\end_layout

\begin_layout Standard
Of course, this scheme only applies when 
\begin_inset Formula $g\left(\widehat{\beta}_{t-1}\right)$
\end_inset

 is differentiable.
 If it is non-differentiable, we do the following
\begin_inset Formula 
\begin{gather*}
\widehat{\beta}_{t}=\arg\min_{\beta,\left\{ z_{i}\right\} _{i=1}^{n}}\rho\left(\widehat{\lambda}_{t-1}'g_{i}\left(\widehat{\beta}_{t-1}\right)+z_{i}\right)\\
\mbox{s.t. }\left|z_{i}\right|\leq\lambda'_{t-1}\mbox{subgradient}\left(\widehat{\beta}-\widehat{\beta}_{t-1}\right)\mbox{ for all }i=1,\ldots,n.
\end{gather*}

\end_inset


\end_layout

\begin_layout Standard
What is the advantage of this updating scheme? 
\end_layout

\begin_layout Standard
The outerloop mimics the idea of Newton-type optimization by giving the
 direction and the step size.
\begin_inset Foot
status open

\begin_layout Plain Layout
If do not want to be too aggressive, we can weaken the step size.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Justification
\end_layout

\begin_layout Standard
If the data is viewed as deterministic, then we can get the optimal point.
\end_layout

\begin_layout Standard
If the data is viewed as random, then this is a boosting scheme and we can
 follow the Golden Chain argument (Bickel, Ritov and Zakai, 2006).
 
\end_layout

\begin_layout Section
Boosting
\end_layout

\begin_layout Standard
The problem is particularly close to a boosting with the exponential loss
 when we take the exponential tilting 
\begin_inset Formula $\rho\left(v\right)=\exp\left(v\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The criterion function 
\begin_inset Formula 
\[
\sum_{i=1}^{n}\left(\widehat{\lambda}_{t-1}'g_{i}\left(\widehat{\beta}_{t-1}\right)+z_{i}\right)=\sum_{i=1}^{n}w_{i}^{t-1}\exp\left(z_{i}\right)
\]

\end_inset

where 
\begin_inset Formula $w_{i}^{t-1}=\exp\left(\widehat{\lambda}_{t-1}'g_{i}\left(\widehat{\beta}_{t-1}\right)\right)$
\end_inset

.
 
\begin_inset Formula $w_{i}^{t-1}$
\end_inset

 is a weight for the 
\begin_inset Formula $i$
\end_inset

-th observation at the point 
\begin_inset Formula $\widehat{\beta}_{t-1}$
\end_inset

.
 Then the weight is updated by 
\begin_inset Formula $\exp\left(z_{i}\right)$
\end_inset

.
 Each iteration adjusts the weight until we have the optimal weight---the
 weight reaches a minimum.
\end_layout

\begin_layout Section
Theory
\end_layout

\begin_layout Standard
So far it is a numerical algorithm.
 Can we do some theory?
\end_layout

\begin_layout Standard
Suppose we start from an efficient GMM estimator, so that we already have
 asymptotic normality with the minimal variance.
 What is the benefit of the iteration? Newey and Smith shows the high-order
 improvement of empirical likelihood by an Edgeworth expansion.
\end_layout

\begin_layout Standard
Here is my conjecture.
 By each updating, the updating will increase the variance.
 But the increment is of very small order of 
\begin_inset Formula $o\left(n^{-1}\right)$
\end_inset

, so that it does not show up in the asymptotic variance when we scale the
 estimator by a factor 
\begin_inset Formula $\sqrt{n}$
\end_inset

.
 However, this explains why the empirical likelihood estimator has a larger
 variance than GMM.
\end_layout

\end_body
\end_document
