#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand\theenumi{(\alph{enumi})}
\renewcommand\labelenumi{\theenumi}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format dvi
\output_sync 1
\output_sync_macro "\synctex=-1"
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\branch abc
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\branch pf of gamma = 0
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Numerical Algorithm for Empirical Likelihood
\end_layout

\begin_layout Author
Zhentao Shi
\end_layout

\begin_layout Standard
The empirical likelihood estimation can be written as 
\begin_inset Formula 
\[
\min_{\beta,\lambda}\sum_{i=1}^{n}\rho\left(\lambda'g_{i}\left(\beta\right)\right)
\]

\end_inset

where 
\begin_inset Formula $\rho\left(\cdot\right)=-\log\left(1+\cdot\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
The empirical likelihood literature well recognizes the difficulty of numerical
 optimization.
 It was suggested (Owen 2001, Kitamura 2006) that we implement the optimization
 in two steps, the inner loop and the outer loop.
 The outer loop is a general nonlinear optimization.
 It may have multiple local minima, while the inner loop is an convex problem.
\end_layout

\begin_layout Standard
I propose in this note one way to reduce the computational burden of outer
 loop function evaluation that requires to solve the inner optimization.
 At any 
\begin_inset Formula $\widehat{\beta}_{t-1}$
\end_inset

, there is a corresponding 
\begin_inset Formula $\widehat{\lambda}_{t-1}$
\end_inset

.
 Let an arbitrary point 
\begin_inset Formula $\widehat{\beta}_{0}$
\end_inset

 be the initial value.
 I suggest the update direction and the step size.
 Let 
\begin_inset Formula 
\[
\widehat{\beta}_{t}=\arg\min_{\beta}\sum_{i=1}^{n}\rho\left(\widehat{\lambda}_{t-1}'g_{i}\left(\widehat{\beta}_{t-1}\right)+\widehat{\lambda}'_{t-1}G_{i}\left(\widehat{\beta}_{t-1}\right)\left(\beta-\widehat{\beta}_{t-1}\right)\right)
\]

\end_inset

where 
\begin_inset Formula $G_{i}\left(\beta\right)=\partial g_{i}\left(\beta\right)/\partial\beta'$
\end_inset

 is the Jacobian.
 The problem is convex in 
\begin_inset Formula $\beta$
\end_inset

, so easy to solve.
\end_layout

\begin_layout Standard
The intuition is to approximate 
\begin_inset Formula $g\left(\widehat{\beta}_{t}\right)$
\end_inset

 by 
\begin_inset Formula $g\left(\beta_{t-1}\right)+G\left(\widehat{\beta}_{t-1}\right)\left(\widehat{\beta}_{t}-\widehat{\beta}_{t-1}\right)$
\end_inset

, a linear approximation.
 We first update 
\begin_inset Formula $\widehat{\beta}_{t-1}$
\end_inset

 to 
\begin_inset Formula $\widehat{\beta}_{t}$
\end_inset

, then update 
\begin_inset Formula $\widehat{\lambda}_{t-1}$
\end_inset

 to 
\begin_inset Formula $\widehat{\lambda}_{t}$
\end_inset

, which is a convex problem as well.
 Next we start another iteration of updating procedure 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Standard
Need to restrict 
\begin_inset Formula $G_{i}\left(\widehat{\beta}_{t-1}\right)$
\end_inset

 so that it does not shoot too far.
\end_layout

\begin_layout Standard
The outerloop mimics the idea of Newton-type optimization by giving the
 direction and the step size.
\begin_inset Foot
status open

\begin_layout Plain Layout
If do not want to be too aggressive, we can weaken the step size.
\end_layout

\end_inset


\end_layout

\end_body
\end_document
